{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Summaries\n",
    "\n",
    "This notebook reads the trained GPT2 model and generates summaries for specified number of articles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "#from googlesearch import search\n",
    "import numpy as np\n",
    "import requests\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "import torch\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "from dataset import GPT21024Dataset \n",
    "from utils import add_special_tokens, beam_search, generate_beam_sample, generate_sample, sample_seq, set_seed, top_k_top_p_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(seed=42, num_workers=4, device=device(type='cuda'), output_dir='./output', model_dir='./weights', root_dir='./pubmed/gpt2_1024_data', ids_file='./pubmed/ids.json')\n"
     ]
    }
   ],
   "source": [
    "#please change default arguments if needed\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--seed\",default=42, type=int,  help=\"seed to replicate results\")\n",
    "parser.add_argument(\"--num_workers\",default=4, type=int,  help=\"num of cpus available\")\n",
    "parser.add_argument(\"--device\",default=torch.device('cuda'), help=\"torch.device object\")\n",
    "parser.add_argument(\"--output_dir\",default='./output', type=str,  help=\"path to save evaluation results\")\n",
    "parser.add_argument(\"--model_dir\",default='./weights', type=str,  help=\"path to save trained model\")\n",
    "parser.add_argument(\"--root_dir\",default='./pubmed/gpt2_1024_data', type=str, help=\"location of json dataset.\")\n",
    "parser.add_argument(\"--ids_file\",default='./pubmed/ids.json', type=str, help=\"location of train, valid and test file indexes\")\n",
    "args = parser.parse_args([])\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the test set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the same validation and training data as during training\n",
    "tokenizer = add_special_tokens()\n",
    "# train_data = GPT21024Dataset(args.root_dir,args.ids_file,mode='train',length=3000)\n",
    "# valid_data = GPT21024Dataset(args.root_dir,args.ids_file,mode='valid',length=500)\n",
    "test_data = GPT21024Dataset(args.root_dir,args.ids_file,mode='test',length=770)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_file and config_file are files used to load finetuned model, change these name as per your file names\n",
    "\n",
    "# model_file = os.path.join(args.model_dir, 'model_data{}_trained_after_{}_epochs_only_sum_loss_ignr_pad.bin'.format(len(train_data),args.num_train_epochs))\n",
    "# config_file = os.path.join(args.model_dir, 'config_data{}_trained_after_{}_epochs_only_sum_loss_ignr_pad.json'.format(len(train_data),args.num_train_epochs))\n",
    "\n",
    "# path to model and config files\n",
    "model_file = os.path.join(args.model_dir, \"model_data6100_trained_after_5_epochs_only_sum_loss_ignr_pad.bin\")\n",
    "config_file = os.path.join(args.model_dir, \"config_data6100_trained_after_5_epochs_only_sum_loss_ignr_pad.json\")\n",
    "\n",
    "config = GPT2Config.from_json_file(config_file)\n",
    "model = GPT2LMHeadModel(config)\n",
    "state_dict = torch.load(model_file)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Results\n",
    "\n",
    "The parameter num decides how many articles we want to generate summary for. \n",
    "\n",
    "For more details, please refer to the docstring of generate_sample function in utils.py\n",
    "\n",
    "The following results were ran using a model trained with batch_size = 3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dae3ecb467c489082854992bc7ba76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_article\n",
      "\n",
      "An RNA polymerase II elongation factor encoded by the human ELL gene. The human ELL gene on chromosome 19 undergoes frequent translocations with the trithorax-like MLL gene on chromosome 11 in acute myeloid leukemias. Here, ELL was shown to encode a previously uncharacterized elongation factor that can increase the catalytic rate of RNA polymerase II transcription by suppressing transient pausing by polymerase at multiple sites along the DNA. Functionally, ELL resembles Elongin (SIII), a transcription elongation factor regulated by the product of the von Hippel-Lindau (VHL) tumor suppressor gene. The discovery of a second elongation factor implicated in oncogenesis provides further support for a close connection between the regulation of transcription elongation and cell growth.\n",
      "\n",
      "generated_summary\n",
      "\n",
      "<|sep|>ELX involved_in RNA polymerase II transcription. The human ELL gene is expressed in several cell lines and is expressed in a variety of tissues. The human ELL gene is expressed in a variety of tissues and is expressed in a variety of cell lines. The ELL gene is expressed in a variety of tissues and is expressed in a variety of cell lines. The human ELL gene is expressed in a variety of cell lines and is expressed in a variety<|pad|>ELL involved_in\n",
      "\n",
      "actual_summary\n",
      "\n",
      "ELL involved_in positive regulation of DNA-templated transcription, elongation. <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59fddd60597e415385af9bb157f3fd45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_article\n",
      "\n",
      "Regulation of phosphorylation of neuronal microtubule-associated proteins MAP1b and MAP2 by protein phosphatase-2A and -2B in rat brain. The function of the neuronal high molecular weight microtubule-associated proteins (MAPs) MAP1b and MAP2 is regulated by the degree of their phosphorylation, which in turn is controlled by the activities of protein kinases and protein phosphatases (PP). To investigate the role of PP in the regulation of the phosphorylation of MAP1b and MAP2, we used okadaic acid and cyclosporin A to selectively inhibit PP2A and PP2B activities, respectively, in metabolically competent rat brain slices. The alteration of the phosphorylation levels of MAP1b and MAP2 was examined by Western blots using several phosphorylation-dependent antibodies to these proteins. The inhibition of PP2A, and to a lesser extent of PP2B, was found to induce an increased phosphorylation of MAP1b and inhibit its microtubule binding activity. Immunocytochemically, a marked increase in neuronal staining in inhibitor-treated tissue was observed with antibodies to the phosphorylated MAP1b. The inhibition of PP2A but not of PP2B also induced phosphorylation of MAP2 at multiple sites and impaired its microtubule binding activity. These results suggest that PP2A might be the major PP that participates in regulation of the phosphorylation of MAP1b and MAP2 and their biological activities.\n",
      "\n",
      "generated_summary\n",
      "\n",
      "<|sep|>MAP1B involved_in regulation of phosphorylation of membrane. MAP1B involved_in regulation of protein phosphatase-2A activity. MAP2 involved_in regulation of phosphorylation of membrane. These results suggest that the regulation of the phosphorylation of MAP1b and MAP2 may be a key component of the regulation of the regulation of the phosphorylation of the membrane.<|sep|>MAP2 involved_in regulation of protein phosphatase-2A activity.\n",
      "\n",
      "actual_summary\n",
      "\n",
      "MAP2 involved_in regulation of microtubule polymerization. <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_sample(test_data, tokenizer, model, num=2, length=100, temperature=1, top_k=10, top_p=0.5, device=args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another function generating summaries. This time with several summaries generated for each article, and have a score attached to each. \n",
    "\n",
    "For more details, please refer to generate_beam_sample function in utils.py. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c44c10a343468a99083b8928c95aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_article\n",
      "\n",
      "An RNA polymerase II elongation factor encoded by the human ELL gene. The human ELL gene on chromosome 19 undergoes frequent translocations with the trithorax-like MLL gene on chromosome 11 in acute myeloid leukemias. Here, ELL was shown to encode a previously uncharacterized elongation factor that can increase the catalytic rate of RNA polymerase II transcription by suppressing transient pausing by polymerase at multiple sites along the DNA. Functionally, ELL resembles Elongin (SIII), a transcription elongation factor regulated by the product of the von Hippel-Lindau (VHL) tumor suppressor gene. The discovery of a second elongation factor implicated in oncogenesis provides further support for a close connection between the regulation of transcription elongation and cell growth\n",
      "\n",
      "actual_summary\n",
      "\n",
      "ELL involved_in positive regulation of DNA-templated transcription, elongation. <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n",
      "\n",
      "generated_summary-1 and Score is 5.851519660031723e-31.\n",
      "\n",
      "<|sep|>ELL enables_ polymerase II transcription factor activity. involved_in transcription polymerase II transcription.- polymerase II enablesation factor activity. The_LL gene_in DNAatingase II.. ELLase II enables_ IIase activity. activity_in DNA polymerase II. elongation factor activity.ation factor activity. TheLL enables_ factor activity. TheLL enables_ factor activity. TheLL enables_ factor activity. Thease activity_in DNA polymerase II.\n",
      "\n",
      "generated_summary-2 and Score is 4.1274501312209414e-32.\n",
      "\n",
      "<|pad|>ELLE involved DNADNApolymerase II factor; The ELL RNA elongation factorase by RNA elongation factor transcription_ase II; involvedLLE involved_inactiv. process. transcription The RNA polymer enables1 polymerase polymer transcription factorator The.dependent RNAXX enables transcription polymerase II1 factor TheLLX_ E_ geneation IIase. E_ation II IIase enables E polymerase II transcription enables transcription ELL enables enables elongation factor transcription. enables\n",
      "\n",
      "generated_summary-3 and Score is 1.1659875402600123e-32.\n",
      "\n",
      " TheILX1 RNA- elongation elongation.. This human E DNA by RNA polymer enables II DNA polymerase II elong polymer. The. E EDNA enables enableson RNA polymer complex_ enables by TheILX DNA is RNA DNA- activ activity.<|sep|>ofIL-22; TheILX transcription elong E_E enablesdependent transcription elong. polymer. II mRNA elong enables DNA polymer enables. DNA_ activator1 factor enables polymer_ transcription transcription transcription elongation activity. transcription\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341476c9bd514fb3bf87ff1fb3375699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_article\n",
      "\n",
      "Regulation of phosphorylation of neuronal microtubule-associated proteins MAP1b and MAP2 by protein phosphatase-2A and -2B in rat brain. The function of the neuronal high molecular weight microtubule-associated proteins (MAPs) MAP1b and MAP2 is regulated by the degree of their phosphorylation, which in turn is controlled by the activities of protein kinases and protein phosphatases (PP). To investigate the role of PP in the regulation of the phosphorylation of MAP1b and MAP2, we used okadaic acid and cyclosporin A to selectively inhibit PP2A and PP2B activities, respectively, in metabolically competent rat brain slices. The alteration of the phosphorylation levels of MAP1b and MAP2 was examined by Western blots using several phosphorylation-dependent antibodies to these proteins. The inhibition of PP2A, and to a lesser extent of PP2B, was found to induce an increased phosphorylation of MAP1b and inhibit its microtubule binding activity. Immunocytochemically, a marked increase in neuronal staining in inhibitor-treated tissue was observed with antibodies to the phosphorylated MAP1b. The inhibition of PP2A but not of PP2B also induced phosphorylation of MAP2 at multiple sites and impaired its microtubule binding activity. These results suggest that PP2A might be the major PP that participates in regulation of the phosphorylation of MAP1b and MAP2 and their biological activities\n",
      "\n",
      "actual_summary\n",
      "\n",
      "MAP2 involved_in regulation of microtubule polymerization. <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n",
      "\n",
      "generated_summary-1 and Score is 1.8589193334548952e-35.\n",
      "\n",
      "<|sep|>MAP2B involved_in regulation ofat phosphorylation..11.1 phosphorylation.A..in.2.orylation._ involved_in regulation of phosphorylation.-2B..........22.at phosphorylation. of.1.1.1.1_1. of ofatatatat.2.1.1.1.1.1.1.1.1\n",
      "\n",
      "generated_summary-2 and Score is 1.0518778897821587e-35.\n",
      "\n",
      "<|pad|>PP1B involved_in regulation of proteinrophosphorylation MAP MAP. enables proteinatase-2B activity_..1.1. involved1_in regulation of protein protein phosphatase1 by.11222b222 MAP1b.1 involved_in regulation1 MAP protein phosphatase-2 involved.in regulation1 phosph phosph phosph phosphase-112 phosph2 of2_2_.11.1..\n",
      "\n",
      "generated_summary-3 and Score is 8.181291449041864e-36.\n",
      "\n",
      " TheP2A enables phosphory protein phosph cell activity.at by; MAP2 MAP phosph.at activity by. involved11 involved1 phosph2A.2in protein phosphatase phosphorylation. activity1 involved involved MAP MAP MAP1 MAP MAP MAP1 MAP MAP. phosph1.2 involved12 of1 involved1. activity1B involved1 involved2 protein the...at activity of of.at_ of of. of2 involved. of2 involved1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_beam_sample(test_data, tokenizer, model, num=2, length=100, beam_size=3, device=args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discoveries/Issues\n",
    "\n",
    "1. Batch_size = 3 seems to be about the upper limit that RTX 3090 could take. It already requires one to restart the kernel each time one wants to re-run the training, otherwise there will be \"CUDA out of memory\" error. \n",
    "2. With batch_size = 3, the training time is shortened to 2/3 comparing to when batch_size = 1, but the result summaries generated looks not as good. There may be a problem with how I calculated the loss values in a batch. \n",
    "3. When batch_size = 3, strangely most summaries generated have a separator token in front. It could also be a result of the problem in my way of calculating loss over a batch. \n",
    "4. When batch_size = 1, the issue is not present. The summaries generated are grammatically correct, but they often have the wrong entity name or fail to capture the exact main idea in the actual summary. Perhaps we do need to somehow specify keywords that the summaries must contain. \n",
    "5. Most of the summaries repeat themselves over 1 or 2 sentences, as if just to fill out all the token spaces. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cells below are irrelevant to our study. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download An Article Given A Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_from_query(query):\n",
    "    # Get url\n",
    "    if query.startswith(\"http\"):\n",
    "        url = query\n",
    "    else:\n",
    "        url = search(query, num_results=1)[0]\n",
    "    print(url)\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page)\n",
    "    # Get text from all <p> tags.\n",
    "    p_tags = soup.find_all('p')\n",
    "    # Get the text from each of the \"p\" tags and strip surrounding whitespace.\n",
    "    p_tags_text = \" \".join([tag.get_text().strip() for tag in p_tags])\n",
    "    return p_tags_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1957 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "article = sentences_from_query(\"neural embedding\")\n",
    "article = tokenizer.encode(article)[:900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa77e633e8b441f9baac6610ed379ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generated_text = sample_seq(model, article, 50, args.device, temperature=1, top_k=10, top_p=0.5)\n",
    "generated_text = generated_text[0, len(article):].tolist()\n",
    "text = tokenizer.convert_ids_to_tokens(generated_text,skip_special_tokens=True)\n",
    "text = tokenizer.convert_tokens_to_string(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: \n",
      "\n",
      "Applications of neural networks have expanded significantly in recent years from image segmentation to natural language processing to time-series forecasting. One notably successful use of deep learning is embedding, a method used to represent discrete variables as continuous vectors. This technique has found practical applications with word embeddings for machine translation and entity embeddings for categorical variables. In this article, I’ll explain what neural network embeddings are, why we want to use them, and how they are learned. We’ll go through these concepts in the context of a real problem I’m working on: representing all the books on Wikipedia as vectors to create a book recommendation system. An embedding is a mapping of a discrete — categorical — variable to a vector of continuous numbers. In the context of neural networks, embeddings are low-dimensional, learned continuous vector representations of discrete variables. Neural network embeddings are useful because they can reduce the dimensionality of categorical variables and meaningfully represent categories in the transformed space. Neural network embeddings have 3 primary purposes: This means in terms of the book project, using neural network embeddings, we can take all 37,000 book articles on Wikipedia and represent each one using only 50 numbers in a vector. Moreover, because embeddings are learned, books that are more similar in the context of our learning problem are closer to one another in the embedding space. Neural network embeddings overcome the two limitations of a common method for representing categorical variables: one-hot encoding. The operation of one-hot encoding categorical variables is actually a simple embedding where each category is mapped to a different vector. This process takes discrete entities and maps each observation to a vector of 0s and a single 1 signaling the specific category. The one-hot encoding technique has two main drawbacks: The first problem is well-understood: for each additional category — referred to as an entity — we have to add another number to the one-hot encoded vector. If we have 37,000 books on Wikipedia, then representing these requires a 37,000-dimensional vector for each book, which makes training any machine learning model on this representation infeasible. The second problem is equally limiting: one-hot encoding does not place similar entities closer to one another in vector space. If we measure similarity between vectors using the cosine distance, then after one-hot encoding, the similarity is 0 for every comparison between entities. This means that entities such as War and Peace and Anna Karenina (both classic books by Leo Tolstoy) are no closer to one another than War and Peace is to The Hitchhiker’s Guide to the Galaxy if we use one-hot encoding. Considering these two problems, the ideal solution for representing categorical variables would require fewer numbers than the number of unique categories and would place similar categories closer to one another. To construct a better representation of categorical entities, we can use an embedding neural network and a supervised task to learn embeddings. The main issue with one-hot encoding is that the transformation does not rely on any supervision. We can greatly improve embeddings by learning them using a neural network on a supervised task. The embeddings form the parameters — weights — of the network which are adjusted to minimize loss on the task. The resulting embedded vectors are representations of categories where similar categories — relative to the task — are closer to one another. For example, if we have a vocabulary of 50,000 words used in a collection of movie reviews, we could learn 100-dimensional embeddings for each word using an embedding neural network trained to predict the sentimentality of the reviews. (For exactly this application see this Google Colab Notebook). Words in the vocabulary that are associated with positive reviews such as “brilliant” or “excellent” will come out closer in the embedding space because the network has learned these are both associated with positive reviews. In the book example given above, our supervised task could be “identify whether or not a book was written by Leo Tolstoy” and the resulting embeddings would place books written by Tolstoy closer to each other. Figuring out how to create the supervised task to produce relevant representations is the toughest part of making embeddings. In the Wikipedia book project (complete notebook here), the supervised learning task is set as predicting\n",
      "------------------------------------------------------------ \n",
      "\n",
      "Generated Summary: \n",
      "\n",
      " the sentiment of each book based on the<|sep|>sentimentality of the book.<|sep|>Learning neural networks<|sep|>Learning neural networks<|sep|>Learning neural networks to predict sentimentality of books is a<|sep|>problem which requires a neural network trained on a supervised task. This\n"
     ]
    }
   ],
   "source": [
    "print(\"Article: \\n\")\n",
    "print(tokenizer.decode(article))\n",
    "print(\"------------------------------------------------------------ \\n\")\n",
    "print(\"Generated Summary: \\n\")\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
