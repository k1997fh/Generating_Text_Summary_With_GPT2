{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "#from googlesearch import search\n",
    "import numpy as np\n",
    "import requests\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "import torch\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "from dataset import GPT21024Dataset \n",
    "from utils import add_special_tokens, beam_search, generate_beam_sample, generate_sample, sample_seq, set_seed, top_k_top_p_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(seed=42, num_workers=4, device=device(type='cuda'), output_dir='./output', model_dir='./weights', root_dir='./pubmed/gpt2_1024_data', ids_file='./pubmed/ids.json')\n"
     ]
    }
   ],
   "source": [
    "#please change default arguments if needed\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--seed\",default=42, type=int,  help=\"seed to replicate results\")\n",
    "parser.add_argument(\"--num_workers\",default=4, type=int,  help=\"num of cpus available\")\n",
    "parser.add_argument(\"--device\",default=torch.device('cuda'), help=\"torch.device object\")\n",
    "parser.add_argument(\"--output_dir\",default='./output', type=str,  help=\"path to save evaluation results\")\n",
    "parser.add_argument(\"--model_dir\",default='./weights', type=str,  help=\"path to save trained model\")\n",
    "parser.add_argument(\"--root_dir\",default='./pubmed/gpt2_1024_data', type=str, help=\"location of json dataset.\")\n",
    "parser.add_argument(\"--ids_file\",default='./pubmed/ids.json', type=str, help=\"location of train, valid and test file indexes\")\n",
    "args = parser.parse_args([])\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the same validation and training data as during training\n",
    "tokenizer = add_special_tokens()\n",
    "# train_data = GPT21024Dataset(args.root_dir,args.ids_file,mode='train',length=3000)\n",
    "# valid_data = GPT21024Dataset(args.root_dir,args.ids_file,mode='valid',length=500)\n",
    "test_data = GPT21024Dataset(args.root_dir,args.ids_file,mode='test',length=770)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_file and config_file are files used to load finetuned model, change these name as per your file names\n",
    "\n",
    "# model_file = os.path.join(args.model_dir, 'model_data{}_trained_after_{}_epochs_only_sum_loss_ignr_pad.bin'.format(len(train_data),args.num_train_epochs))\n",
    "# config_file = os.path.join(args.model_dir, 'config_data{}_trained_after_{}_epochs_only_sum_loss_ignr_pad.json'.format(len(train_data),args.num_train_epochs))\n",
    "\n",
    "# path to model and config files\n",
    "model_file = os.path.join(args.model_dir, \"model_data6100_trained_after_5_epochs_only_sum_loss_ignr_pad.bin\")\n",
    "config_file = os.path.join(args.model_dir, \"config_data6100_trained_after_5_epochs_only_sum_loss_ignr_pad.json\")\n",
    "\n",
    "config = GPT2Config.from_json_file(config_file)\n",
    "model = GPT2LMHeadModel(config)\n",
    "state_dict = torch.load(model_file)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada23ed5391e482aa3199156f6dc2886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_article\n",
      "\n",
      "The human ELL gene on chromosome 19 undergoes frequent translocations with the trithorax-like MLL gene on chromosome 11 in acute myeloid leukemias. Here, ELL was shown to encode a previously uncharacterized elongation factor that can increase the catalytic rate of RNA polymerase II transcription by suppressing transient pausing by polymerase at multiple sites along the DNA. Functionally, ELL resembles Elongin (SIII), a transcription elongation factor regulated by the product of the von Hippel-Lindau (VHL) tumor suppressor gene. The discovery of a second elongation factor implicated in oncogenesis provides further support for a close connection between the regulation of transcription elongation and cell growth.\n",
      "\n",
      "generated_summary\n",
      "\n",
      " ELL enables DNA-binding transcription factor activity. ELL involved_in transcription by RNA polymerase II. ELL involved_in DNA-binding transcription factor activity. ELL involved_in DNA-binding transcription factor activity. ELL involved_in DNA-binding transcription factor activity. ELL involved_in DNA-binding transcription factor activity. ELL involved_in DNA-binding transcription factor activity. ELL involved_in DNA-binding transcription factor activity. ELL involved_in DNA\n",
      "\n",
      "actual_summary\n",
      "\n",
      "An RNA polymerase II elongation factor encoded by the human ELL gene.ELL involved_in positive regulation of DNA-templated transcription, elongation. <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f6777296cc49759f80159a4878c3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_article\n",
      "\n",
      "The function of the neuronal high molecular weight microtubule-associated proteins (MAPs) MAP1b and MAP2 is regulated by the degree of their phosphorylation, which in turn is controlled by the activities of protein kinases and protein phosphatases (PP). To investigate the role of PP in the regulation of the phosphorylation of MAP1b and MAP2, we used okadaic acid and cyclosporin A to selectively inhibit PP2A and PP2B activities, respectively, in metabolically competent rat brain slices. The alteration of the phosphorylation levels of MAP1b and MAP2 was examined by Western blots using several phosphorylation-dependent antibodies to these proteins. The inhibition of PP2A, and to a lesser extent of PP2B, was found to induce an increased phosphorylation of MAP1b and inhibit its microtubule binding activity. Immunocytochemically, a marked increase in neuronal staining in inhibitor-treated tissue was observed with antibodies to the phosphorylated MAP1b. The inhibition of PP2A but not of PP2B also induced phosphorylation of MAP2 at multiple sites and impaired its microtubule binding activity. These results suggest that PP2A might be the major PP that participates in regulation of the phosphorylation of MAP1b and MAP2 and their biological activities.\n",
      "\n",
      "generated_summary\n",
      "\n",
      " MAP2 involved_in regulation of cell population proliferation. MAP2 involved_in regulation of cell population proliferation. MAP2 involved_in regulation of cell population proliferation. MAP2 involved_in regulation of apoptotic process. MAP2 involved_in regulation of apoptotic process. MAP2 involved_in regulation of apoptotic process. MAP2 involved_in regulation of apoptotic process. MAP2 involved_in regulation of apoptotic process. MAP2 involved_in regulation of apoptotic process. MAP\n",
      "\n",
      "actual_summary\n",
      "\n",
      "Regulation of phosphorylation of neuronal microtubule-associated proteins MAP1b and MAP2 by protein phosphatase-2A and -2B in rat brain.MAP2 involved_in regulation of microtubule polymerization. <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_sample(test_data, tokenizer, model, num=2, length=100, temperature=1, top_k=10, top_p=0.5, device=args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0889b7184a6c474c89ac1d5afd03ecb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_article\n",
      "\n",
      "The human ELL gene on chromosome 19 undergoes frequent translocations with the trithorax-like MLL gene on chromosome 11 in acute myeloid leukemias. Here, ELL was shown to encode a previously uncharacterized elongation factor that can increase the catalytic rate of RNA polymerase II transcription by suppressing transient pausing by polymerase at multiple sites along the DNA. Functionally, ELL resembles Elongin (SIII), a transcription elongation factor regulated by the product of the von Hippel-Lindau (VHL) tumor suppressor gene. The discovery of a second elongation factor implicated in oncogenesis provides further support for a close connection between the regulation of transcription elongation and cell growth\n",
      "\n",
      "actual_summary\n",
      "\n",
      "An RNA polymerase II elongation factor encoded by the human ELL gene.ELL involved_in positive regulation of DNA-templated transcription, elongation. <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n",
      "\n",
      "generated_summary-1 and Score is 4.710319526472805e-25.\n",
      "\n",
      " ELL involved_in transcription polymerase II activity.. inLL involved_in transcription polymerase II activity. involved_in_ elongation._ination_factor.inLL involved_in transcription ofase polymerase II._in_ation_factor involved_in transcription elongation activity._in__ation_factor involved_in transcription elongation. elongation involved_in regulation ofation elongation. involved_in regulation of transcription elongation. involved_in regulation\n",
      "\n",
      "generated_summary-2 and Score is 3.9413838918629947e-26.\n",
      "\n",
      "\n",
      "<|pad|> enables involved involved DNA elongation factorase activity involvedELL. involved_ RNA elongation factor factor;ELL involved transcription binding. factor transcription elong_factor involved_ELL involved_in regulation polymeration.ELL involved enables DNA elong DNA binding..ELL regulation ofase. activityELL involvedation_of DNA factor complex. regulation_of transcriptionELL factor complex. transcription elong transcription.ELLE involved_in regulation of transcription elongation.LLLL involved\n",
      "\n",
      "generated_summary-3 and Score is 3.0157591575799476e-26.\n",
      "\n",
      "ELLA1_ RNA by RNA polymer.;; Eactiv1 enables DNA DNA-binding transcription activity.ELL enables RNAin binding of1.E.of_CD__1DNA DNA elong RNA.ELL;ELL enablescanonical;EF1 mRNA polymer transcription factorE.ELLcanon factorin transcription.ELL DNA of E by_CD_1 involved_in cell factor involved; ELL enables transcription elong cell adhesion; EE involved transcription\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34ad445949e40239ce5aeea6b3579c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_article\n",
      "\n",
      "The function of the neuronal high molecular weight microtubule-associated proteins (MAPs) MAP1b and MAP2 is regulated by the degree of their phosphorylation, which in turn is controlled by the activities of protein kinases and protein phosphatases (PP). To investigate the role of PP in the regulation of the phosphorylation of MAP1b and MAP2, we used okadaic acid and cyclosporin A to selectively inhibit PP2A and PP2B activities, respectively, in metabolically competent rat brain slices. The alteration of the phosphorylation levels of MAP1b and MAP2 was examined by Western blots using several phosphorylation-dependent antibodies to these proteins. The inhibition of PP2A, and to a lesser extent of PP2B, was found to induce an increased phosphorylation of MAP1b and inhibit its microtubule binding activity. Immunocytochemically, a marked increase in neuronal staining in inhibitor-treated tissue was observed with antibodies to the phosphorylated MAP1b. The inhibition of PP2A but not of PP2B also induced phosphorylation of MAP2 at multiple sites and impaired its microtubule binding activity. These results suggest that PP2A might be the major PP that participates in regulation of the phosphorylation of MAP1b and MAP2 and their biological activities\n",
      "\n",
      "actual_summary\n",
      "\n",
      "Regulation of phosphorylation of neuronal microtubule-associated proteins MAP1b and MAP2 by protein phosphatase-2A and -2B in rat brain.MAP2 involved_in regulation of microtubule polymerization. <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n",
      "\n",
      "generated_summary-1 and Score is 4.785181783456641e-22.\n",
      "\n",
      "PP2A involved_in regulation ofmembrane-mediated protein ofstream.MAPMAP involved_inin of oforyory.MAP__2 involved_in regulation of protein surfacecellase activity.MAP involved_in regulation of protein-containing complex assembly.MAP involved_in regulation of protein-containing complex assembly.MAP involved_in regulation of protein-containing complex assembly.MAP involved_in regulation of of phosphorylation.MAP involved_in regulation of protein-containing complex\n",
      "\n",
      "generated_summary-2 and Score is 1.567992946670114e-22.\n",
      "\n",
      " MAP2B__MAP protein transorylation regulated transport of_ folding protein assembly.22 involved_ regulation regulation phosphorganic generation; involved involved2MAP2B_MAP protein cell- receptor binding..MAP2Bin_MAP cell phosphorylation regulatedlation.2 involved_in regulation cell-cell signaling.MAP2B involved_in regulation cell-cell signaling.MAP2B involved_in regulation protein protein.MAP2B involved_in regulation of cell-cell signaling\n",
      "\n",
      "generated_summary-3 and Score is 2.886416493110353e-23.\n",
      "\n",
      "P1 involved involvedMAP2 signal phosph protein binding. receptor involved transportup proteins activity.11 involved_in protein protein phosph- generation.MAPMAP in part_of protein complex membrane phosphat signaling pathwayMAP2 involved_ involved_in2-phosphory; involved1 involved_ protein protein of surface receptor complex pathwayMAP32 involved_in protein of membrane phosphat pathway; involved2 involved_in protein cell folding phosphory by MAP1.MAP protein protein phosphorylation regulated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_beam_sample(test_data, tokenizer, model, num=2, length=100, beam_size=3, device=args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download An Article Given A Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_from_query(query):\n",
    "    # Get url\n",
    "    if query.startswith(\"http\"):\n",
    "        url = query\n",
    "    else:\n",
    "        url = search(query, num_results=1)[0]\n",
    "    print(url)\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page)\n",
    "    # Get text from all <p> tags.\n",
    "    p_tags = soup.find_all('p')\n",
    "    # Get the text from each of the \"p\" tags and strip surrounding whitespace.\n",
    "    p_tags_text = \" \".join([tag.get_text().strip() for tag in p_tags])\n",
    "    return p_tags_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1957 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "article = sentences_from_query(\"neural embedding\")\n",
    "article = tokenizer.encode(article)[:900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa77e633e8b441f9baac6610ed379ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generated_text = sample_seq(model, article, 50, args.device, temperature=1, top_k=10, top_p=0.5)\n",
    "generated_text = generated_text[0, len(article):].tolist()\n",
    "text = tokenizer.convert_ids_to_tokens(generated_text,skip_special_tokens=True)\n",
    "text = tokenizer.convert_tokens_to_string(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: \n",
      "\n",
      "Applications of neural networks have expanded significantly in recent years from image segmentation to natural language processing to time-series forecasting. One notably successful use of deep learning is embedding, a method used to represent discrete variables as continuous vectors. This technique has found practical applications with word embeddings for machine translation and entity embeddings for categorical variables. In this article, I’ll explain what neural network embeddings are, why we want to use them, and how they are learned. We’ll go through these concepts in the context of a real problem I’m working on: representing all the books on Wikipedia as vectors to create a book recommendation system. An embedding is a mapping of a discrete — categorical — variable to a vector of continuous numbers. In the context of neural networks, embeddings are low-dimensional, learned continuous vector representations of discrete variables. Neural network embeddings are useful because they can reduce the dimensionality of categorical variables and meaningfully represent categories in the transformed space. Neural network embeddings have 3 primary purposes: This means in terms of the book project, using neural network embeddings, we can take all 37,000 book articles on Wikipedia and represent each one using only 50 numbers in a vector. Moreover, because embeddings are learned, books that are more similar in the context of our learning problem are closer to one another in the embedding space. Neural network embeddings overcome the two limitations of a common method for representing categorical variables: one-hot encoding. The operation of one-hot encoding categorical variables is actually a simple embedding where each category is mapped to a different vector. This process takes discrete entities and maps each observation to a vector of 0s and a single 1 signaling the specific category. The one-hot encoding technique has two main drawbacks: The first problem is well-understood: for each additional category — referred to as an entity — we have to add another number to the one-hot encoded vector. If we have 37,000 books on Wikipedia, then representing these requires a 37,000-dimensional vector for each book, which makes training any machine learning model on this representation infeasible. The second problem is equally limiting: one-hot encoding does not place similar entities closer to one another in vector space. If we measure similarity between vectors using the cosine distance, then after one-hot encoding, the similarity is 0 for every comparison between entities. This means that entities such as War and Peace and Anna Karenina (both classic books by Leo Tolstoy) are no closer to one another than War and Peace is to The Hitchhiker’s Guide to the Galaxy if we use one-hot encoding. Considering these two problems, the ideal solution for representing categorical variables would require fewer numbers than the number of unique categories and would place similar categories closer to one another. To construct a better representation of categorical entities, we can use an embedding neural network and a supervised task to learn embeddings. The main issue with one-hot encoding is that the transformation does not rely on any supervision. We can greatly improve embeddings by learning them using a neural network on a supervised task. The embeddings form the parameters — weights — of the network which are adjusted to minimize loss on the task. The resulting embedded vectors are representations of categories where similar categories — relative to the task — are closer to one another. For example, if we have a vocabulary of 50,000 words used in a collection of movie reviews, we could learn 100-dimensional embeddings for each word using an embedding neural network trained to predict the sentimentality of the reviews. (For exactly this application see this Google Colab Notebook). Words in the vocabulary that are associated with positive reviews such as “brilliant” or “excellent” will come out closer in the embedding space because the network has learned these are both associated with positive reviews. In the book example given above, our supervised task could be “identify whether or not a book was written by Leo Tolstoy” and the resulting embeddings would place books written by Tolstoy closer to each other. Figuring out how to create the supervised task to produce relevant representations is the toughest part of making embeddings. In the Wikipedia book project (complete notebook here), the supervised learning task is set as predicting\n",
      "------------------------------------------------------------ \n",
      "\n",
      "Generated Summary: \n",
      "\n",
      " the sentiment of each book based on the<|sep|>sentimentality of the book.<|sep|>Learning neural networks<|sep|>Learning neural networks<|sep|>Learning neural networks to predict sentimentality of books is a<|sep|>problem which requires a neural network trained on a supervised task. This\n"
     ]
    }
   ],
   "source": [
    "print(\"Article: \\n\")\n",
    "print(tokenizer.decode(article))\n",
    "print(\"------------------------------------------------------------ \\n\")\n",
    "print(\"Generated Summary: \\n\")\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
