{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel,AdamW, WarmupLinearSchedule\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "from dataset import GPT21024Dataset \n",
    "from utils import add_special_tokens, generate_sample, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7652, 5)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"pubmed_gene_go_summary.tsv\"\n",
    "path = os.path.join(\"pubmed\", filename)\n",
    "pubmed_table = pd.read_csv(path, sep = \"\\t\")\n",
    "pubmed_table = pubmed_table[pubmed_table['Abstract'].notna()]\n",
    "pubmed_table = pubmed_table[pubmed_table['Target'].notna()]\n",
    "pubmed_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GeneID</th>\n",
       "      <th>PubMed</th>\n",
       "      <th>Target</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>14668352</td>\n",
       "      <td>SERPINA3 enables DNA binding; located_in nucleus.</td>\n",
       "      <td>The SANT2 domain of the murine tumor cell DnaJ...</td>\n",
       "      <td>The murine tumor cell DnaJ-like protein 1 or M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64215</td>\n",
       "      <td>14668352</td>\n",
       "      <td>DNAJC1 enables ATPase activator activity; invo...</td>\n",
       "      <td>The SANT2 domain of the murine tumor cell DnaJ...</td>\n",
       "      <td>The murine tumor cell DnaJ-like protein 1 or M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>2404007</td>\n",
       "      <td>SERPINA3 enables serine-type endopeptidase inh...</td>\n",
       "      <td>Cloning, expression, purification, and biologi...</td>\n",
       "      <td>Human alpha 1-antichymotrypsin has been cloned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>11481320</td>\n",
       "      <td>AADAC enables lipase activity.</td>\n",
       "      <td>Characterization of the rodent genes for aryla...</td>\n",
       "      <td>In the current study, we have determined the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>8063807</td>\n",
       "      <td>AADAC enables catalytic activity.</td>\n",
       "      <td>Human liver arylacetamide deacetylase. Molecul...</td>\n",
       "      <td>Microsomal arylacetamide deacetylase (DAC) com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GeneID    PubMed                                             Target  \\\n",
       "0      12  14668352  SERPINA3 enables DNA binding; located_in nucleus.   \n",
       "1   64215  14668352  DNAJC1 enables ATPase activator activity; invo...   \n",
       "2      12   2404007  SERPINA3 enables serine-type endopeptidase inh...   \n",
       "3      13  11481320                     AADAC enables lipase activity.   \n",
       "4      13   8063807                  AADAC enables catalytic activity.   \n",
       "\n",
       "                                               Title  \\\n",
       "0  The SANT2 domain of the murine tumor cell DnaJ...   \n",
       "1  The SANT2 domain of the murine tumor cell DnaJ...   \n",
       "2  Cloning, expression, purification, and biologi...   \n",
       "3  Characterization of the rodent genes for aryla...   \n",
       "4  Human liver arylacetamide deacetylase. Molecul...   \n",
       "\n",
       "                                            Abstract  \n",
       "0  The murine tumor cell DnaJ-like protein 1 or M...  \n",
       "1  The murine tumor cell DnaJ-like protein 1 or M...  \n",
       "2  Human alpha 1-antichymotrypsin has been cloned...  \n",
       "3  In the current study, we have determined the c...  \n",
       "4  Microsomal arylacetamide deacetylase (DAC) com...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubmed_table.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(i,article, abstract):\n",
    "    \"\"\" Saves a json file.\"\"\"\n",
    "\n",
    "    file = \"./pubmed/gpt2_1024_data/\"+str(i)+\".json\"\n",
    "    js_example = {}\n",
    "    js_example['id'] = i\n",
    "    js_example['article'] = article\n",
    "    js_example['abstract'] = abstract\n",
    "    with open(file, 'w') as f:\n",
    "        json.dump(js_example, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeneID                                                    183\n",
      "PubMed                                               17159080\n",
      "Target      AGT involved_in G protein-coupled receptor sig...\n",
      "Title       Cross-talk between angiotensin II receptor typ...\n",
      "Abstract                                                  NaN\n",
      "Name: 274, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(pubmed_table.iloc[274])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  files written\n",
      "100  files written\n",
      "200  files written\n",
      "300  files written\n",
      "400  files written\n",
      "500  files written\n",
      "600  files written\n",
      "700  files written\n",
      "800  files written\n",
      "900  files written\n",
      "1000  files written\n",
      "1100  files written\n",
      "1200  files written\n",
      "1400  files written\n",
      "1500  files written\n",
      "1600  files written\n",
      "1700  files written\n",
      "1800  files written\n",
      "1900  files written\n",
      "2000  files written\n",
      "2100  files written\n",
      "2200  files written\n",
      "2300  files written\n",
      "2400  files written\n",
      "2500  files written\n",
      "2600  files written\n",
      "2700  files written\n",
      "2800  files written\n",
      "2900  files written\n",
      "3000  files written\n",
      "3100  files written\n",
      "3200  files written\n",
      "3300  files written\n",
      "3400  files written\n",
      "3500  files written\n",
      "3600  files written\n",
      "3700  files written\n",
      "3800  files written\n",
      "3900  files written\n",
      "4000  files written\n",
      "4100  files written\n",
      "4200  files written\n",
      "4300  files written\n",
      "4400  files written\n",
      "4500  files written\n",
      "4600  files written\n",
      "4700  files written\n",
      "4800  files written\n",
      "4900  files written\n",
      "5000  files written\n",
      "5100  files written\n",
      "5200  files written\n",
      "5300  files written\n",
      "5400  files written\n",
      "5500  files written\n",
      "5600  files written\n",
      "5700  files written\n",
      "5800  files written\n",
      "5900  files written\n",
      "6000  files written\n",
      "6100  files written\n",
      "6200  files written\n",
      "6300  files written\n",
      "6400  files written\n",
      "6500  files written\n",
      "6600  files written\n",
      "6700  files written\n",
      "6800  files written\n",
      "6900  files written\n",
      "7000  files written\n",
      "7100  files written\n",
      "7200  files written\n",
      "7300  files written\n",
      "7400  files written\n",
      "7500  files written\n",
      "7600  files written\n",
      "7700  files written\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# file_id_map maps the json file ids to actual cnn/dm file names ending with \".story\"\\nprint(\"saving file_id_map...\")\\n\\nwith open(\"file_id_map.pickle\", \\'wb\\') as f:\\n    pickle.dump(file_id_map,f)\\nprint(\"file_id_map saved.\")\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = add_special_tokens()\n",
    "train_ids = []\n",
    "#file_id_map = {}\n",
    "for index, row in pubmed_table.iterrows():\n",
    "    article = row[\"Abstract\"]\n",
    "    summary = row[\"Target\"]\n",
    "    article = tokenizer.encode(article)\n",
    "    summary = tokenizer.encode(summary)\n",
    "    if len(article) > 0 and len(summary) > 0 and (len(article) + len(summary) <= 1023):\n",
    "        train_ids.append(index)\n",
    "        write_json(index, article, summary)\n",
    "        #file_id_map[i] = os.path.basename(file).replace('.story', '')\n",
    "        if index%100==0:\n",
    "            print(index, \" files written\")\n",
    "            \n",
    "x,y = int(len(train_ids)*0.8), int(len(train_ids)*0.9)\n",
    "valid_ids = train_ids[x:y]\n",
    "test_ids = train_ids[y:]\n",
    "train_ids = train_ids[:x]\n",
    "with open(\"./pubmed/ids.json\",'w') as f:\n",
    "    js = {}\n",
    "    js['train_ids'] = train_ids\n",
    "    js['valid_ids'] = valid_ids\n",
    "    js['test_ids'] = test_ids\n",
    "    json.dump(js,f)\n",
    "\n",
    "\"\"\"\n",
    "# file_id_map maps the json file ids to actual cnn/dm file names ending with \".story\"\n",
    "print(\"saving file_id_map...\")\n",
    "\n",
    "with open(\"file_id_map.pickle\", 'wb') as f:\n",
    "    pickle.dump(file_id_map,f)\n",
    "print(\"file_id_map saved.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(lr=5e-05, seed=42, n_gpu=1, gradient_accumulation_steps=2, batch_size=1, num_workers=4, device=device(type='cpu'), num_train_epochs=1, output_dir='./output', model_dir='./weights', max_grad_norm=1.0, root_dir='./pubmed/gpt2_1024_data', ids_file='./pubmed/ids.json')\n"
     ]
    }
   ],
   "source": [
    "#please change default arguments if needed\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--lr\",default=5e-5, type=float, help=\"learning rate\")\n",
    "parser.add_argument(\"--seed\",default=42, type=int,  help=\"seed to replicate results\")\n",
    "parser.add_argument(\"--n_gpu\",default=1, type=int,  help=\"no of gpu available\")\n",
    "parser.add_argument(\"--gradient_accumulation_steps\",default=2, type=int, help=\"gradient_accumulation_steps\")\n",
    "parser.add_argument(\"--batch_size\",default=1, type=int,  help=\"batch_size\")\n",
    "parser.add_argument(\"--num_workers\",default=4, type=int,  help=\"num of cpus available\")\n",
    "parser.add_argument(\"--device\",default=torch.device('cpu'), help=\"torch.device object\")\n",
    "parser.add_argument(\"--num_train_epochs\",default=1, type=int,  help=\"no of epochs of training\")\n",
    "parser.add_argument(\"--output_dir\",default='./output', type=str,  help=\"path to save evaluation results\")\n",
    "parser.add_argument(\"--model_dir\",default='./weights', type=str,  help=\"path to save trained model\")\n",
    "parser.add_argument(\"--max_grad_norm\",default=1.0, type=float, help=\"max gradient norm.\")\n",
    "parser.add_argument(\"--root_dir\",default='./pubmed/gpt2_1024_data', type=str, help=\"location of json dataset.\")\n",
    "parser.add_argument(\"--ids_file\",default='./pubmed/ids.json', type=str, help=\"location of train, valid and test file indexes\")\n",
    "args = parser.parse_args([])\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, tokenizer, train_dataset, valid_dataset, ignore_index):\n",
    "    \"\"\" Trains GPT2 model and logs necessary details.\n",
    "        Args:\n",
    "            args: dict that contains all the necessary information passed by user while training\n",
    "            model: finetuned gpt/gpt2 model\n",
    "            tokenizer: GPT/GPT2 tokenizer\n",
    "            train_dataset: GPT21024Dataset object for training data\n",
    "            ignore_index: token not considered in loss calculation\n",
    "    \"\"\"\n",
    "    writer = SummaryWriter('./output/logs')\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dl = DataLoader(train_dataset,sampler=train_sampler,batch_size=args.batch_size,num_workers=args.num_workers)\n",
    "    loss_fct = CrossEntropyLoss(ignore_index=ignore_index) #ignores padding token for loss calculation\n",
    "    optimizer = AdamW(model.parameters(),lr=args.lr)\n",
    "    scheduler = WarmupLinearSchedule(optimizer,100,80000)\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = tnrange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "    set_seed(args)\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm_notebook(train_dl, desc=\"Training\")\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            inputs, labels = batch['article'].to(args.device), batch['article'].to(args.device)\n",
    "            model.train()\n",
    "            logits = model(inputs)[0]\n",
    "            # only consider loss on reference summary just like seq2seq models\n",
    "            shift_logits = logits[..., batch['sum_idx']:-1, :].contiguous()\n",
    "            shift_labels = labels[..., batch['sum_idx']+1:].contiguous()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            loss = loss/args.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "                writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
    "                writer.add_scalar('loss', (tr_loss - logging_loss)/args.gradient_accumulation_steps, global_step)\n",
    "                logging_loss = tr_loss\n",
    "                print(\"loss:\", loss.item(), end='\\n\\n')\n",
    "                if (step + 1)/args.gradient_accumulation_steps == 1.0:\n",
    "                \tprint('After 1st update: ', end='\\n\\n')\n",
    "                \tgenerate_sample(valid_dataset, tokenizer, model, num=2, eval_step=False,device=args.device)\n",
    "                \n",
    "                \n",
    "            if (step + 1) % (10*args.gradient_accumulation_steps) == 0:\n",
    "                results = evaluate(args, model, valid_dataset, ignore_index, global_step)\n",
    "                for key, value in results.items():\n",
    "                    writer.add_scalar('eval_{}'.format(key), value, global_step)\n",
    "                print('After', global_step+1,'updates: ', end='\\n\\n')\n",
    "                generate_sample(valid_dataset, tokenizer, model, num=2, eval_step=True,device=args.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, model, eval_dataset, ignore_index, global_step=None):\n",
    "    \"\"\" Returns perplexity score on validation dataset.\n",
    "        Args:\n",
    "            args: dict that contains all the necessary information passed by user while training\n",
    "            model: finetuned gpt/gpt2 model\n",
    "            eval_dataset: GPT21024Dataset object for validation data\n",
    "            global_step: no. of times gradients have backpropagated\n",
    "            ignore_index: token not considered in loss calculation\n",
    "    \"\"\"\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.mkdir(args.output_dir)\n",
    "    eval_output_dir = args.output_dir\n",
    "\n",
    "    results = {}\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.batch_size)\n",
    "    loss_fct = CrossEntropyLoss(ignore_index=ignore_index) #ignores padding token for loss calculation\n",
    "\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n",
    "        inputs, labels = batch['article'].to(args.device), batch['article'].to(args.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs)[0]\n",
    "            idx = batch['sum_idx'].item() # index of separator token\n",
    "            # only consider loss on reference summary just like seq2seq models\n",
    "            shift_logits = logits[..., batch['sum_idx']:-1, :].contiguous()\n",
    "            shift_labels = labels[..., batch['sum_idx']+1:].contiguous()\n",
    "            lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    result = {\n",
    "        \"perplexity\": perplexity\n",
    "    }\n",
    "    print(\"perplexity:\", perplexity.item())\n",
    "\n",
    "    if global_step:\n",
    "        output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"a\") as f:\n",
    "            for key in sorted(result.keys()):\n",
    "                f.write('\\n\\n')\n",
    "                f.write(\"time = %s, %s = %s, step = %s\\n\" % (datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"), key, str(result[key]), str(global_step)))\n",
    "    return result           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating training and validation dataset object\n",
    "\n",
    "train_data = GPT21024Dataset(args.root_dir,args.ids_file,mode='train',length=3000) #training on only 3000 datasets\n",
    "valid_data = GPT21024Dataset(args.root_dir,args.ids_file,mode='valid',length=500)  #validation on only 500 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 665/665 [00:00<00:00, 322265.99B/s]\n",
      "100%|████████████████████████| 548118077/548118077 [00:34<00:00, 16069161.70B/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pretrained GPT2\n",
    "tokenizer = add_special_tokens()\n",
    "ignore_idx = tokenizer.pad_token_id\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3148508/2934305216.py:20: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  train_iterator = tnrange(int(args.num_train_epochs), desc=\"Epoch\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c09c737672848dea9d620ab5802ef11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3148508/2934305216.py:23: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  epoch_iterator = tqdm_notebook(train_dl, desc=\"Training\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3bf565f6ed4a17b6d474413113965b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rli10/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 29.225767135620117\n",
      "\n",
      "After 1st update: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rli10/anaconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1cfdd7eb8e4745a7a7f1bb3a54e092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_article\n",
      "\n",
      "We report the cDNA deduced primary structure of a wheat germ agglutinin-reactive nuclear pore complex (NPC) protein of rat. The protein, termed Nup98 (for nucleoporin of 98 kDa), contains numerous GLFG and FG repeats and some FXFG repeats and is thus a vertebrate member of a family of GLFG nucleoporins that were previously discovered in yeast. Immunoelectron microscopy showed Nup98 to be asymmetrically located at the nucleoplasmic side of the NPC. Nup98 functions as one of several docking site nucleoporins in a cytosolic docking activity-mediated binding of a model transport substrate. The docking site of Nup98 was mapped to its N-terminal half, which contains all of the peptide repeats. A recombinant segment of this region depleted the docking activity of cytosol. We suggest that the peptide repeat domain of Nup98, together with peptide repeat domains of other nucleoporins, forms an array of sites for mediated docking of transport substrate, and that bidirectional transport across the NPC proceeds by repeated docking and undocking reactions.\n",
      "\n",
      "generated_summary\n",
      "\n",
      "<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|sep|><|pad|><|pad|><|sep|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|sep|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|sep|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n",
      "\n",
      "actual_summary\n",
      "\n",
      "NUP98 enables transporter activity. <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539f8a2501914829a1483e1a73f9ad46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_article\n",
      "\n",
      "The cellular effects of stromal cell-derived factor-1 (SDF-1) are mediated primarily by binding to the CXC chemokine receptor-4. We report in this study that SDF-1 and its peptide analogues induce a concentration- and time-dependent accumulation of phosphatidylinositol-(3,4,5)-trisphosphate (PtdIns(3,4,5)P3) in Jurkat cells. This SDF-1-stimulated generation of D-3 phosphoinositide lipids was inhibited by pretreatment of the cells with an SDF-1 peptide antagonist or an anti-CXCR4 Ab. In addition, the phosphoinositide 3 (PI 3)-kinase inhibitors wortmannin and LY294002, as well as the Gi protein inhibitor pertussis toxin, also inhibited the SDF-1-stimulated accumulation of PtdIns(3,4,5)P3. The effects of SDF-1 on D-3 phosphoinositide lipid accumulation correlated well with activation of the known PI 3-kinase effector protein kinase B, which was also inhibited by wortmannin and pertussis toxin. Concentrations of PI 3-kinase inhibitors, sufficient to inhibit PtdIns(3,4,5)P3 accumulation, also inhibited chemotaxis of Jurkat and peripheral blood-derived T lymphocytes in response to SDF-1. In contrast, SDF-1-stimulated actin polymerization was only partially inhibited by PI 3-kinase inhibitors, suggesting that while chemotaxis is fully dependent on PI 3-kinase activation, actin polymerization requires additional biochemical inputs. Finally, SDF-1-stimulated extracellular signal-related kinase (ERK)-1/2 mitogen-activated protein kinase activation was inhibited by PI 3-kinase inhibitors. In addition, the mitogen-activated protein/ERK kinase inhibitor PD098059 partially attenuated chemotaxis in response to SDF-1. Hence, it appears that ERK1/2 activation is dependent on PI 3-kinase activation, and both biochemical events are involved in the regulation of SDF-1-stimulated chemotaxis.\n",
      "\n",
      "generated_summary\n",
      "\n",
      "<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>!<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|sep|><|sep|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>!<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|sep|><|pad|><|pad|><|pad|><|sep|><|pad|><|pad|><|sep|><|pad|><|pad|><|pad|><|pad|><|pad|><|sep|><|pad|><|pad|><|pad|><|pad|>\n",
      "<|sep|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n",
      "\n",
      "actual_summary\n",
      "\n",
      "AKT1 enables protein kinase activity; involved_in G protein-coupled receptor signaling pathway. <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n",
      "\n",
      "loss: 35.73088073730469\n",
      "\n",
      "loss: 36.04054641723633\n",
      "\n",
      "loss: 39.15319061279297\n",
      "\n",
      "loss: 42.15578842163086\n",
      "\n",
      "loss: 43.608219146728516\n",
      "\n",
      "loss: 45.455257415771484\n",
      "\n",
      "loss: 31.81484603881836\n",
      "\n",
      "loss: 39.612342834472656\n",
      "\n",
      "loss: 29.910985946655273\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7131b118c41480ba05c738af92568ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2.906761187957438e+30\n",
      "After 11 updates: \n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "generate_sample() missing 1 required positional argument: 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3148508/3076434413.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total time: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" minutes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3148508/2934305216.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, tokenizer, train_dataset, valid_dataset, ignore_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m                     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eval_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'After'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'updates: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0mgenerate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: generate_sample() missing 1 required positional argument: 'model'"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "\n",
    "start = time.time()\n",
    "train(args, model, tokenizer, train_data, valid_data, ignore_idx)\n",
    "print('total time: ', (time.time()-start)/60, \" minutes\", end='\\n\\n')\n",
    "\n",
    "print('Saving trained model...')\n",
    "model_file = os.path.join(args.model_dir, 'model_data{}_trained_after_{}_epochs_only_sum_loss_ignr_pad.bin'.format(len(train_data),args.num_train_epochs))\n",
    "config_file = os.path.join(args.model_dir, 'config_data{}_trained_after_{}_epochs_only_sum_loss_ignr_pad.json'.format(len(train_data),args.num_train_epochs))\n",
    "torch.save(model.state_dict(), model_file)\n",
    "model.config.to_json_file(config_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
